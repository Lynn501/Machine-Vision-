{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "KEUIQRs4ugzw",
   "metadata": {
    "id": "KEUIQRs4ugzw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "kNE-nS5CvfNX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kNE-nS5CvfNX",
    "outputId": "8606b83a-2285-4c83-c31a-c2c73a85e6c4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_dir = r'/content/drive/MyDrive/cars_tanks/train'\n",
    "# test_dir = r'/content/drive/MyDrive/cars_tanks/test'\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "import zipfile \n",
    "import os \n",
    "files = zipfile.ZipFile('cars_tanks.zip', 'r') \n",
    "files.extractall(os.getcwd()) \n",
    "files.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "pXEXsERHulcL",
   "metadata": {
    "id": "pXEXsERHulcL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, stride=1, downsample=None, **kwargs):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=out_channel,\n",
    "                               kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channel)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel,\n",
    "                               kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "A51_Qkayut_a",
   "metadata": {
    "id": "A51_Qkayut_a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, stride=1, downsample=None,\n",
    "                 groups=1, width_per_group=64):\n",
    "        super(Bottleneck, self).__init__()\n",
    "\n",
    "        width = int(out_channel * (width_per_group / 64.)) * groups\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=width,\n",
    "                               kernel_size=1, stride=1, bias=False)  # squeeze channels\n",
    "        self.bn1 = nn.BatchNorm2d(width)\n",
    "        # -----------------------------------------\n",
    "        self.conv2 = nn.Conv2d(in_channels=width, out_channels=width, groups=groups,\n",
    "                               kernel_size=3, stride=stride, bias=False, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(width)\n",
    "        # -----------------------------------------\n",
    "        self.conv3 = nn.Conv2d(in_channels=width, out_channels=out_channel*self.expansion,\n",
    "                               kernel_size=1, stride=1, bias=False)  # unsqueeze channels\n",
    "        self.bn3 = nn.BatchNorm2d(out_channel*self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "IxSDcJ9tuq46",
   "metadata": {
    "id": "IxSDcJ9tuq46",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GoogLeNet(nn.Module):\n",
    "    # init()：进行初始化，申明模型中各层的定义\n",
    "    # num_classes：需要分类的类别个数\n",
    "    # aux_logits：训练过程是否使用辅助分类器，init_weights：是否对网络进行权重初始化\n",
    "    def __init__(self, num_classes=1000, aux_logits=True, init_weights=False):\n",
    "        super(GoogLeNet, self).__init__()\n",
    "        self.aux_logits = aux_logits\n",
    "\n",
    "        self.conv1 = BasicConv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        # ceil_mode=true时，将不够池化的数据自动补足NAN至kernel_size大小\n",
    "        self.maxpool1 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.conv2 = BasicConv2d(64, 64, kernel_size=1)\n",
    "        self.conv3 = BasicConv2d(64, 192, kernel_size=3, padding=1)\n",
    "        self.maxpool2 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.inception3a = Inception(192, 64, 96, 128, 16, 32, 32)\n",
    "        self.inception3b = Inception(256, 128, 128, 192, 32, 96, 64)\n",
    "        self.maxpool3 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.inception4a = Inception(480, 192, 96, 208, 16, 48, 64)\n",
    "        self.inception4b = Inception(512, 160, 112, 224, 24, 64, 64)\n",
    "        self.inception4c = Inception(512, 128, 128, 256, 24, 64, 64)\n",
    "        self.inception4d = Inception(512, 112, 144, 288, 32, 64, 64)\n",
    "        self.inception4e = Inception(528, 256, 160, 320, 32, 128, 128)\n",
    "        self.maxpool4 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.inception5a = Inception(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.inception5b = Inception(832, 384, 192, 384, 48, 128, 128)\n",
    "\n",
    "        # 如果为真，则使用分类器\n",
    "        if self.aux_logits:\n",
    "            self.aux1 = InceptionAux(512, num_classes)\n",
    "            self.aux2 = InceptionAux(528, num_classes)\n",
    "        # AdaptiveAvgPool2d：自适应平均池化，指定输出（H，W）\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc = nn.Linear(1024, num_classes)\n",
    "        # 如果为真，则对网络参数进行初始化\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    # forward()：定义前向传播过程,描述了各层之间的连接关系\n",
    "    def forward(self, x):\n",
    "        # N x 3 x 224 x 224\n",
    "        x = self.conv1(x)\n",
    "        # N x 64 x 112 x 112\n",
    "        x = self.maxpool1(x)\n",
    "        # N x 64 x 56 x 56\n",
    "        x = self.conv2(x)\n",
    "        # N x 64 x 56 x 56\n",
    "        x = self.conv3(x)\n",
    "        # N x 192 x 56 x 56\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        # N x 192 x 28 x 28\n",
    "        x = self.inception3a(x)\n",
    "        # N x 256 x 28 x 28\n",
    "        x = self.inception3b(x)\n",
    "        # N x 480 x 28 x 28\n",
    "        x = self.maxpool3(x)\n",
    "        # N x 480 x 14 x 14\n",
    "        x = self.inception4a(x)\n",
    "        # N x 512 x 14 x 14\n",
    "        # 设置.train()时为训练模式，self.training=True\n",
    "        if self.training and self.aux_logits:\n",
    "            aux1 = self.aux1(x)\n",
    "\n",
    "        x = self.inception4b(x)\n",
    "        # N x 512 x 14 x 14\n",
    "        x = self.inception4c(x)\n",
    "        # N x 512 x 14 x 14\n",
    "        x = self.inception4d(x)\n",
    "        # N x 528 x 14 x 14\n",
    "        if self.training and self.aux_logits:\n",
    "            aux2 = self.aux2(x)\n",
    "\n",
    "        x = self.inception4e(x)\n",
    "        # N x 832 x 14 x 14\n",
    "        x = self.maxpool4(x)\n",
    "        # N x 832 x 7 x 7\n",
    "        x = self.inception5a(x)\n",
    "        # N x 832 x 7 x 7\n",
    "        x = self.inception5b(x)\n",
    "        # N x 1024 x 7 x 7\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        # N x 1024 x 1 x 1\n",
    "        x = torch.flatten(x, 1)\n",
    "        # N x 1024\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        # N x 1000 (num_classes)\n",
    "        if self.training and self.aux_logits:\n",
    "            return x, aux2, aux1\n",
    "        return x\n",
    "\n",
    "    # 网络结构参数初始化\n",
    "    def _initialize_weights(self):\n",
    "        # 遍历网络中的每一层\n",
    "        for m in self.modules():\n",
    "            # isinstance(object, type)，如果指定的对象拥有指定的类型，则isinstance()函数返回True\n",
    "            # 如果是卷积层\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                # Kaiming正态分布方式的权重初始化\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                # 如果偏置不是0，将偏置置成0，对偏置进行初始化\n",
    "                if m.bias is not None:\n",
    "                    # torch.nn.init.constant_(tensor, val)，初始化整个矩阵为常数val\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            # 如果是全连接层\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                # init.normal_(tensor, mean=0.0, std=1.0)，使用从正态分布中提取的值填充输入张量\n",
    "                # 参数：tensor：一个n维Tensor，mean：正态分布的平均值，std：正态分布的标准差\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "# 基础卷积层（卷积 + ReLU）\n",
    "class BasicConv2d(nn.Module):\n",
    "    # init()：进行初始化，申明模型中各层的定义\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, **kwargs)\n",
    "        # ReLU(inplace=True)：将tensor直接修改，不找变量做中间的传递，节省运算内存，不用多存储额外的变量\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    # 前向传播过程\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "# Inception结构\n",
    "class Inception(nn.Module):\n",
    "    # init()：进行初始化\n",
    "    def __init__(self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj):\n",
    "        super(Inception, self).__init__()\n",
    "        # 分支1，单1x1卷积层\n",
    "        self.branch1 = BasicConv2d(in_channels, ch1x1, kernel_size=1)\n",
    "        # 分支2，1x1卷积层后接3x3卷积层\n",
    "        self.branch2 = nn.Sequential(\n",
    "            BasicConv2d(in_channels, ch3x3red, kernel_size=1),\n",
    "            # 保证输出大小等于输入大小\n",
    "            BasicConv2d(ch3x3red, ch3x3, kernel_size=3, padding=1)\n",
    "        )\n",
    "        # 分支3，1x1卷积层后接5x5卷积层\n",
    "        self.branch3 = nn.Sequential(\n",
    "            BasicConv2d(in_channels, ch5x5red, kernel_size=1),\n",
    "            # 保证输出大小等于输入大小\n",
    "            BasicConv2d(ch5x5red, ch5x5, kernel_size=5, padding=2)\n",
    "        )\n",
    "        # 分支4，3x3最大池化层后接1x1卷积层\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            BasicConv2d(in_channels, pool_proj, kernel_size=1)\n",
    "        )\n",
    "    # forward()：定义前向传播过程,描述了各层之间的连接关系\n",
    "    def forward(self, x):\n",
    "        branch1 = self.branch1(x)\n",
    "        branch2 = self.branch2(x)\n",
    "        branch3 = self.branch3(x)\n",
    "        branch4 = self.branch4(x)\n",
    "\n",
    "        # 在通道维上连结输出\n",
    "        outputs = [branch1, branch2, branch3, branch4]\n",
    "        # cat()：在给定维度上对输入的张量序列进行连接操作\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "# 辅助分类器\n",
    "class InceptionAux(nn.Module):\n",
    "    # init()：进行初始化\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(InceptionAux, self).__init__()\n",
    "        self.averagePool = nn.AvgPool2d(kernel_size=5, stride=3)\n",
    "        self.conv = BasicConv2d(in_channels, 128, kernel_size=1)\n",
    "        # 上一层output[batch, 128, 4, 4]，128X4X4=2048\n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "\n",
    "    # 前向传播过程\n",
    "    def forward(self, x):\n",
    "        # 输入：分类器1：Nx512x14x14，分类器2：Nx528x14x14\n",
    "        x = self.averagePool(x)\n",
    "        # 输入：分类器1：Nx512x14x14，分类器2：Nx528x14x14\n",
    "        x = self.conv(x)\n",
    "        # 输入：N x 128 x 4 x 4\n",
    "        x = torch.flatten(x, 1)\n",
    "        # 设置.train()时为训练模式，self.training=True\n",
    "        x = F.dropout(x, 0.5, training=self.training)\n",
    "        # 输入：N x 2048\n",
    "        x = F.relu(self.fc1(x), inplace=True)\n",
    "        x = F.dropout(x, 0.5, training=self.training)\n",
    "        # 输入：N x 1024\n",
    "        x = self.fc2(x)\n",
    "        # 返回值：N*num_classes\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "SXDfGe1o6mpC",
   "metadata": {
    "id": "SXDfGe1o6mpC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tankCarCls_v0(num_classes=2, include_top=True):\n",
    "    return tankCarCls(Bottleneck, [3, 4, 6, 3], num_classes=num_classes, include_top=include_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "uAv96sxEuydN",
   "metadata": {
    "id": "uAv96sxEuydN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class tankCarCls(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 block,\n",
    "                 blocks_num,\n",
    "                 num_classes=1000,\n",
    "                 include_top=True,\n",
    "                 groups=1,\n",
    "                 width_per_group=64):\n",
    "        super(tankCarCls, self).__init__()\n",
    "        self.include_top = include_top\n",
    "        self.in_channel = 64\n",
    "\n",
    "        self.groups = groups\n",
    "        self.width_per_group = width_per_group\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, self.in_channel, kernel_size=7, stride=2,\n",
    "                               padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_channel)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, blocks_num[0])\n",
    "        self.layer2 = self._make_layer(block, 128, blocks_num[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, blocks_num[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, blocks_num[3], stride=2)\n",
    "        if self.include_top:\n",
    "            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # output size = (1, 1)\n",
    "            self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    def _make_layer(self, block, channel, block_num, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channel != channel * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channel, channel * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(channel * block.expansion))\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channel,\n",
    "                            channel,\n",
    "                            downsample=downsample,\n",
    "                            stride=stride,\n",
    "                            groups=self.groups,\n",
    "                            width_per_group=self.width_per_group))\n",
    "        self.in_channel = channel * block.expansion\n",
    "\n",
    "        for _ in range(1, block_num):\n",
    "            layers.append(block(self.in_channel,\n",
    "                                channel,\n",
    "                                groups=self.groups,\n",
    "                                width_per_group=self.width_per_group))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        if self.include_top:\n",
    "            x = self.avgpool(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb_hGutHu_qz",
   "metadata": {
    "id": "bb_hGutHu_qz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    import os\n",
    "    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # 下面老是报错 shape 不一致\n",
    "\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    # device = torch.device(\"cpu\")\n",
    "    print(\"using {} device.\".format(device))\n",
    "\n",
    "    data_transform = {\n",
    "        \"train\": transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                                     transforms.RandomHorizontalFlip(),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
    "        \"val\": transforms.Compose([transforms.Resize(256),\n",
    "                                   transforms.CenterCrop(224),\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])}\n",
    "\n",
    "    data_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))  # get data root path\n",
    "    train_dataset = datasets.ImageFolder(root=os.path.join(\"cars_tanks/train\"),\n",
    "                                         transform=data_transform[\"train\"])\n",
    "    train_num = len(train_dataset)\n",
    "\n",
    "\n",
    "    flower_list = train_dataset.class_to_idx\n",
    "    cla_dict = dict((val, key) for key, val in flower_list.items())\n",
    "    # write dict into json file\n",
    "    json_str = json.dumps(cla_dict, indent=2)\n",
    "    with open('class_indices.json', 'w') as json_file:\n",
    "        json_file.write(json_str)\n",
    "\n",
    "    batch_size = 4\n",
    "    nw = 0  # number of workers\n",
    "    print('Using {} dataloader workers every process'.format(nw))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=batch_size, shuffle=True,\n",
    "                                               num_workers=nw)\n",
    "\n",
    "    validate_dataset = datasets.ImageFolder(root=os.path.join(\"cars_tanks/train\"),\n",
    "                                            transform=data_transform[\"val\"])\n",
    "    val_num = len(validate_dataset)\n",
    "    validate_loader = torch.utils.data.DataLoader(validate_dataset,\n",
    "                                                  batch_size=batch_size, shuffle=False,\n",
    "                                                  num_workers=nw)\n",
    "\n",
    "    print(\"using {} images for training, {} images for validation.\".format(train_num,\n",
    "                                                                           val_num))\n",
    "\n",
    "    net = tankCarCls_v0()\n",
    "\n",
    "    # change fc layer structure\n",
    "    in_channel = net.fc.in_features\n",
    "    net.fc = nn.Linear(in_channel, 2)\n",
    "    net.to(device)\n",
    "\n",
    "    # define loss function\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    # construct an optimizer\n",
    "    params = [p for p in net.parameters() if p.requires_grad]\n",
    "    optimizer = optim.Adam(params, lr=0.0001)\n",
    "\n",
    "    epochs = 30\n",
    "    best_acc = 0.0\n",
    "    save_path = './tankCarCls_v1.pth'\n",
    "    train_steps = len(train_loader)\n",
    "    torch.cuda.empty_cache()\n",
    "    writer = SummaryWriter(\"logs\")\n",
    "    for epoch in range(epochs):\n",
    "        # train\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        train_bar = tqdm(train_loader, file=sys.stdout)\n",
    "        for step, data in enumerate(train_bar):\n",
    "            images, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            logits = net(images.to(device))\n",
    "            loss = loss_function(logits, labels.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            train_bar.desc = \"train epoch[{}/{}] loss:{:.3f}\".format(epoch + 1,\n",
    "                                                                     epochs,\n",
    "                                                                     loss)\n",
    "\n",
    "        # validate\n",
    "        net.eval()\n",
    "        acc = 0.0  # accumulate accurate number / epoch\n",
    "        TP = 0.0\n",
    "        FP = 0.0\n",
    "        FN = 0.0\n",
    "        TN = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_bar = tqdm(validate_loader, file=sys.stdout)\n",
    "            for val_data in val_bar:\n",
    "                val_images, val_labels = val_data\n",
    "                outputs = net(val_images.to(device))\n",
    "                # loss = loss_function(outputs, test_labels)\n",
    "                predict_y = torch.max(outputs, dim=1)[1]\n",
    "\n",
    "\n",
    "                acc += torch.eq(predict_y, val_labels.to(device)).sum().item()\n",
    "\n",
    "\n",
    "                TP += (torch.eq(predict_y, val_labels.to(device)) & torch.eq(predict_y, 0)).sum().item()\n",
    "\n",
    "                # 10\n",
    "                FP += (torch.eq(predict_y, 1) & torch.eq(val_labels.to(device), 0)).sum().item()\n",
    "                # 01\n",
    "                FN += (torch.eq(predict_y, 0) & torch.eq(val_labels.to(device), 1)).sum().item()\n",
    "                # 11\n",
    "                TN += (torch.eq(predict_y, 1) & torch.eq(val_labels.to(device), 1)).sum().item()\n",
    "\n",
    "                val_bar.desc = \"valid epoch[{}/{}]\".format(epoch + 1,\n",
    "                                                           epochs)\n",
    "\n",
    "        val_accurate = acc / val_num  #（TP+TN）/（TP+FP+TN+FN）\n",
    "        # Precision=TP/（TP+FP）\n",
    "        Precision = TP /(TP + FP)\n",
    "        Recall = TP / (TP + FN)\n",
    "        # Precision-recall\n",
    "        # Confusion matrix\n",
    "        F1 = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "        Sensitivity = TP / (TP + FN)\n",
    "        Specificity = TN / (TN + FP)\n",
    "        print('[epoch %d] loss: %.3f  accuracy: %.3f  Precision: %.3f  Recall: %.3f  F1: %3f  Sensitivity: %.3f  Specificity: %.3f'   %\n",
    "              (epoch + 1, running_loss / train_steps, val_accurate,Precision,Recall, F1, Sensitivity, Specificity))\n",
    "\n",
    "        writer.add_scalar(\"Accuracy-epoch\", val_accurate, epoch+1)\n",
    "        writer.add_scalar(\"loss-epoch\", running_loss, epoch+1)\n",
    "        writer.add_scalar(\"Precision-epoch\", Precision, epoch+1)\n",
    "        writer.add_scalar(\"Recall-epoch\", Recall, epoch+1)\n",
    "        writer.add_scalar(\"Precision-Recall\", Precision, Recall)\n",
    "        writer.add_scalar(\"F1-epoch\", F1, epoch + 1)\n",
    "        writer.add_scalar(\"Sensitivity-epoch\", Sensitivity, epoch + 1)\n",
    "        writer.add_scalar(\"Specificity-epoch\", Specificity, epoch + 1)\n",
    "        writer.add_scalar(\"Sensitivity-Specificity\", Sensitivity, Specificity)\n",
    "\n",
    "\n",
    "\n",
    "        if val_accurate > best_acc:\n",
    "            best_acc = val_accurate\n",
    "            torch.save(net.state_dict(), save_path)\n",
    "    writer.close()\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ddbdd0a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "id": "9ddbdd0a",
    "outputId": "3815d888-59ac-4fea-8131-92a91453ba30",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda:0 device.\n",
      "Using 0 dataloader workers every process\n",
      "using 1303 images for training, 1303 images for validation.\n",
      "train epoch[1/30] loss:0.529: 100%|██████████| 326/326 [00:19<00:00, 17.06it/s]\n",
      "valid epoch[1/30]: 100%|██████████| 326/326 [00:05<00:00, 57.11it/s]\n",
      "[epoch 1] loss: 0.683  accuracy: 0.653  Precision: 0.981  Recall: 0.622  F1: 0.761352  Sensitivity: 0.622  Specificity: 0.903\n",
      "train epoch[2/30] loss:1.168: 100%|██████████| 326/326 [00:17<00:00, 18.79it/s]\n",
      "valid epoch[2/30]: 100%|██████████| 326/326 [00:05<00:00, 56.86it/s]\n",
      "[epoch 2] loss: 0.630  accuracy: 0.729  Precision: 0.707  Recall: 0.790  F1: 0.746590  Sensitivity: 0.790  Specificity: 0.667\n",
      "train epoch[3/30] loss:1.385: 100%|██████████| 326/326 [00:18<00:00, 18.01it/s]\n",
      "valid epoch[3/30]: 100%|██████████| 326/326 [00:06<00:00, 54.11it/s]\n",
      "[epoch 3] loss: 0.587  accuracy: 0.706  Precision: 0.567  Recall: 0.865  F1: 0.685292  Sensitivity: 0.865  Specificity: 0.613\n",
      "train epoch[4/30] loss:0.654: 100%|██████████| 326/326 [00:17<00:00, 18.98it/s]\n",
      "valid epoch[4/30]: 100%|██████████| 326/326 [00:05<00:00, 59.06it/s]\n",
      "[epoch 4] loss: 0.582  accuracy: 0.827  Precision: 0.871  Recall: 0.831  F1: 0.850498  Sensitivity: 0.831  Specificity: 0.822\n",
      "train epoch[5/30] loss:0.969: 100%|██████████| 326/326 [00:17<00:00, 18.88it/s]\n",
      "valid epoch[5/30]: 100%|██████████| 326/326 [00:05<00:00, 56.32it/s]\n",
      "[epoch 5] loss: 0.578  accuracy: 0.814  Precision: 0.924  Recall: 0.784  F1: 0.848220  Sensitivity: 0.784  Specificity: 0.872\n",
      "train epoch[6/30] loss:0.724: 100%|██████████| 326/326 [00:17<00:00, 18.80it/s]\n",
      "valid epoch[6/30]: 100%|██████████| 326/326 [00:05<00:00, 58.77it/s]\n",
      "[epoch 6] loss: 0.548  accuracy: 0.821  Precision: 0.876  Recall: 0.819  F1: 0.846811  Sensitivity: 0.819  Specificity: 0.824\n",
      "train epoch[7/30] loss:0.324: 100%|██████████| 326/326 [00:17<00:00, 18.91it/s]\n",
      "valid epoch[7/30]: 100%|██████████| 326/326 [00:05<00:00, 58.17it/s]\n",
      "[epoch 7] loss: 0.540  accuracy: 0.800  Precision: 0.954  Recall: 0.755  F1: 0.843055  Sensitivity: 0.755  Specificity: 0.909\n",
      "train epoch[8/30] loss:1.142: 100%|██████████| 326/326 [00:17<00:00, 19.01it/s]\n",
      "valid epoch[8/30]: 100%|██████████| 326/326 [00:05<00:00, 57.94it/s]\n",
      "[epoch 8] loss: 0.530  accuracy: 0.842  Precision: 0.922  Recall: 0.820  F1: 0.868118  Sensitivity: 0.820  Specificity: 0.880\n",
      "train epoch[9/30] loss:0.481: 100%|██████████| 326/326 [00:17<00:00, 19.00it/s]\n",
      "valid epoch[9/30]: 100%|██████████| 326/326 [00:05<00:00, 58.88it/s]\n",
      "[epoch 9] loss: 0.525  accuracy: 0.839  Precision: 0.933  Recall: 0.810  F1: 0.867257  Sensitivity: 0.810  Specificity: 0.893\n",
      "train epoch[10/30] loss:0.285: 100%|██████████| 326/326 [00:17<00:00, 18.63it/s]\n",
      "valid epoch[10/30]: 100%|██████████| 326/326 [00:05<00:00, 55.76it/s]\n",
      "[epoch 10] loss: 0.489  accuracy: 0.835  Precision: 0.849  Recall: 0.857  F1: 0.853042  Sensitivity: 0.857  Specificity: 0.807\n",
      "train epoch[11/30] loss:0.877: 100%|██████████| 326/326 [00:17<00:00, 19.06it/s]\n",
      "valid epoch[11/30]: 100%|██████████| 326/326 [00:05<00:00, 57.85it/s]\n",
      "[epoch 11] loss: 0.489  accuracy: 0.828  Precision: 0.784  Recall: 0.899  F1: 0.837209  Sensitivity: 0.899  Specificity: 0.760\n",
      "train epoch[12/30] loss:0.330: 100%|██████████| 326/326 [00:17<00:00, 19.09it/s]\n",
      "valid epoch[12/30]: 100%|██████████| 326/326 [00:05<00:00, 55.79it/s]\n",
      "[epoch 12] loss: 0.462  accuracy: 0.814  Precision: 0.728  Recall: 0.927  F1: 0.815549  Sensitivity: 0.927  Specificity: 0.725\n",
      "train epoch[13/30] loss:0.196: 100%|██████████| 326/326 [00:17<00:00, 18.71it/s]\n",
      "valid epoch[13/30]: 100%|██████████| 326/326 [00:05<00:00, 57.63it/s]\n",
      "[epoch 13] loss: 0.451  accuracy: 0.850  Precision: 0.913  Recall: 0.836  F1: 0.872562  Sensitivity: 0.836  Specificity: 0.872\n",
      "train epoch[14/30] loss:0.866: 100%|██████████| 326/326 [00:18<00:00, 18.01it/s]\n",
      "valid epoch[14/30]: 100%|██████████| 326/326 [00:05<00:00, 55.40it/s]\n",
      "[epoch 14] loss: 0.456  accuracy: 0.834  Precision: 0.966  Recall: 0.788  F1: 0.867971  Sensitivity: 0.788  Specificity: 0.938\n",
      "train epoch[15/30] loss:1.281: 100%|██████████| 326/326 [00:17<00:00, 18.71it/s]\n",
      "valid epoch[15/30]: 100%|██████████| 326/326 [00:05<00:00, 57.30it/s]\n",
      "[epoch 15] loss: 0.440  accuracy: 0.853  Precision: 0.954  Recall: 0.816  F1: 0.879548  Sensitivity: 0.816  Specificity: 0.923\n",
      "train epoch[16/30] loss:0.127: 100%|██████████| 326/326 [00:17<00:00, 19.10it/s]\n",
      "valid epoch[16/30]: 100%|██████████| 326/326 [00:06<00:00, 54.07it/s]\n",
      "[epoch 16] loss: 0.435  accuracy: 0.869  Precision: 0.857  Recall: 0.905  F1: 0.880503  Sensitivity: 0.905  Specificity: 0.827\n",
      "train epoch[17/30] loss:1.200: 100%|██████████| 326/326 [00:17<00:00, 18.46it/s]\n",
      "valid epoch[17/30]: 100%|██████████| 326/326 [00:05<00:00, 57.73it/s]\n",
      "[epoch 17] loss: 0.430  accuracy: 0.866  Precision: 0.844  Recall: 0.913  F1: 0.876945  Sensitivity: 0.913  Specificity: 0.816\n",
      "train epoch[18/30] loss:0.287: 100%|██████████| 326/326 [00:17<00:00, 19.01it/s]\n",
      "valid epoch[18/30]: 100%|██████████| 326/326 [00:05<00:00, 54.83it/s]\n",
      "[epoch 18] loss: 0.435  accuracy: 0.882  Precision: 0.922  Recall: 0.875  F1: 0.898013  Sensitivity: 0.875  Specificity: 0.892\n",
      "train epoch[19/30] loss:0.973: 100%|██████████| 326/326 [00:17<00:00, 18.47it/s]\n",
      "valid epoch[19/30]: 100%|██████████| 326/326 [00:05<00:00, 56.38it/s]\n",
      "[epoch 19] loss: 0.439  accuracy: 0.830  Precision: 0.959  Recall: 0.787  F1: 0.864500  Sensitivity: 0.787  Specificity: 0.926\n",
      "train epoch[20/30] loss:2.219: 100%|██████████| 326/326 [00:17<00:00, 18.51it/s]\n",
      "valid epoch[20/30]: 100%|██████████| 326/326 [00:06<00:00, 54.06it/s]\n",
      "[epoch 20] loss: 0.408  accuracy: 0.856  Precision: 0.816  Recall: 0.919  F1: 0.864553  Sensitivity: 0.919  Specificity: 0.792\n",
      "train epoch[21/30] loss:0.182: 100%|██████████| 326/326 [00:17<00:00, 18.74it/s]\n",
      "valid epoch[21/30]: 100%|██████████| 326/326 [00:05<00:00, 58.18it/s]\n",
      "[epoch 21] loss: 0.431  accuracy: 0.873  Precision: 0.906  Recall: 0.874  F1: 0.889780  Sensitivity: 0.874  Specificity: 0.872\n",
      "train epoch[22/30] loss:0.818: 100%|██████████| 326/326 [00:17<00:00, 18.93it/s]\n",
      "valid epoch[22/30]: 100%|██████████| 326/326 [00:05<00:00, 58.50it/s]\n",
      "[epoch 22] loss: 0.423  accuracy: 0.886  Precision: 0.884  Recall: 0.912  F1: 0.897790  Sensitivity: 0.912  Specificity: 0.856\n",
      "train epoch[23/30] loss:2.126: 100%|██████████| 326/326 [00:16<00:00, 19.46it/s]\n",
      "valid epoch[23/30]: 100%|██████████| 326/326 [00:05<00:00, 59.83it/s]\n",
      "[epoch 23] loss: 0.421  accuracy: 0.893  Precision: 0.910  Recall: 0.902  F1: 0.905890  Sensitivity: 0.902  Specificity: 0.882\n",
      "train epoch[24/30] loss:0.253: 100%|██████████| 326/326 [00:17<00:00, 19.04it/s]\n",
      "valid epoch[24/30]: 100%|██████████| 326/326 [00:05<00:00, 58.09it/s]\n",
      "[epoch 24] loss: 0.389  accuracy: 0.900  Precision: 0.958  Recall: 0.877  F1: 0.915475  Sensitivity: 0.877  Specificity: 0.938\n",
      "train epoch[25/30] loss:0.228: 100%|██████████| 326/326 [00:18<00:00, 17.88it/s]\n",
      "valid epoch[25/30]: 100%|██████████| 326/326 [00:05<00:00, 56.44it/s]\n",
      "[epoch 25] loss: 0.398  accuracy: 0.858  Precision: 0.977  Recall: 0.810  F1: 0.885873  Sensitivity: 0.810  Specificity: 0.959\n",
      "train epoch[26/30] loss:0.104: 100%|██████████| 326/326 [00:17<00:00, 18.92it/s]\n",
      "valid epoch[26/30]: 100%|██████████| 326/326 [00:05<00:00, 57.91it/s]\n",
      "[epoch 26] loss: 0.404  accuracy: 0.906  Precision: 0.925  Recall: 0.909  F1: 0.917060  Sensitivity: 0.909  Specificity: 0.901\n",
      "train epoch[27/30] loss:0.162: 100%|██████████| 326/326 [00:17<00:00, 18.74it/s]\n",
      "valid epoch[27/30]: 100%|██████████| 326/326 [00:05<00:00, 58.11it/s]\n",
      "[epoch 27] loss: 0.372  accuracy: 0.864  Precision: 0.816  Recall: 0.935  F1: 0.871460  Sensitivity: 0.935  Specificity: 0.796\n",
      "train epoch[28/30] loss:1.721: 100%|██████████| 326/326 [00:17<00:00, 19.05it/s]\n",
      "valid epoch[28/30]: 100%|██████████| 326/326 [00:06<00:00, 54.09it/s]\n",
      "[epoch 28] loss: 0.419  accuracy: 0.875  Precision: 0.947  Recall: 0.849  F1: 0.895177  Sensitivity: 0.849  Specificity: 0.919\n",
      "train epoch[29/30] loss:0.205: 100%|██████████| 326/326 [00:17<00:00, 18.96it/s]\n",
      "valid epoch[29/30]: 100%|██████████| 326/326 [00:05<00:00, 57.10it/s]\n",
      "[epoch 29] loss: 0.359  accuracy: 0.905  Precision: 0.886  Recall: 0.942  F1: 0.913043  Sensitivity: 0.942  Specificity: 0.863\n",
      "train epoch[30/30] loss:0.164: 100%|██████████| 326/326 [00:17<00:00, 18.82it/s]\n",
      "valid epoch[30/30]: 100%|██████████| 326/326 [00:05<00:00, 58.01it/s]\n",
      "[epoch 30] loss: 0.386  accuracy: 0.906  Precision: 0.959  Recall: 0.885  F1: 0.920366  Sensitivity: 0.885  Specificity: 0.941\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
